### Instructions to Run the Application Using Docker Compose

1. **Prerequisites**:
   - Install Docker and Docker Compose from [here](https://www.docker.com/get-started).
   - **Redis Requirement**:
     - **Option 1**: Install Redis locally. [Installation guide here](https://redis.io/docs/getting-started/installation/).
     - **Option 2**: Use Docker to run Redis (included in Docker Compose setup).
    

2. **Install Node.js Dependencies**:
   ```bash
   npm install
   ```

3. **Run Docker Compose**:
   ```bash
   docker-compose up -d
   ```

4. **Access the Application**:
   - Open a web browser or API client and go to:
     ```
     http://127.0.0.1:3000
     ```

5. **Stopping the Application**:
   - Use the following command to stop the services:
   ```bash
   docker-compose down
   ```


---

### Some Confusions:
- Since the Groups feature is in the Pro version of BullMQ, I attempted to mimic a similar feature by creating counters for the number of processes executed in one minute and preventing two tasks from the same user from running in less than one second.
- We could enhance this by using the sliding window technique with Redis's range types.
- Initially, I was uncertain about the type of rate limiting the assignment required—whether it was route rate limiting, worker rate limiting, or queuing rate limiting.

### Assumptions:
1. I have assumed that queuing can be done indefinitely.
2. Rate limiting is implemented in the `worker.ts` file of the task queue.
3. Additionally, if any task is rate-limited, it will be requeued with a two-second delay.

--- 



### Directory Structure Explanation

- **`.env`, `.gitignore`, `docker-compose.yml`, `dockerfile`, `package.json`, `package-lock.json`, `tsconfig.json`**: 
  - These files manage environment variables, Git settings, Docker configurations, project dependencies, and TypeScript configurations. They help ensure smooth project setup and environment-specific configurations.

- **`app.ts`, `index.ts`**: 
  - These files are the main entry points for the application.
  - `app.ts` likely handles the app’s middleware, routes, and services, while `index.ts` could be a central module for organizing imports and exports.

- **Files like `config.ts`, `redis.ts`, `worker.ts`, `routes.ts`, etc.**:
  - **`config.ts`**: This file likely contains application-specific configurations, such as environment settings, API keys, or database connection strings. It's responsible for centralizing configuration values, making the app easier to manage and maintain.

  - **`redis.ts`**: This file manages the Redis client, which is used for tasks like caching or rate-limiting. It likely initializes the connection to the Redis server and exports a reusable instance throughout the application for tasks such as storing rate-limiting data, managing sessions, or handling queues.

  - **`worker.ts`**: his file contains the logic for executing background tasks. In your case, it might involve consuming tasks from a BullMQ queue, processing those tasks, and applying any business rules like rate-limiting. The worker continuously listens for new jobs, processes them, and handles the task's lifecycle (e.g., completion, failure).

  - **`routes.ts`**: This file typically defines routes related to specific resources or features. It maps incoming HTTP requests to corresponding controllers or services. In a larger system, `routes.ts` might define endpoints like `/users` or `/tasks`, and it's later imported into a central route management file that aggregates all the application's routes. This structure helps modularize the routing logic.

- **`UserTask.controller.ts` and `UserTask.service.ts`**:
  - These files contain the API endpoint logic and the business logic for handling user tasks.

- **`task-logs.txt`**:
  - Stores the logs of task executions, generated by the worker process.

### My Approach to the Problem

1. **Directory Organization**: I structured the project into logical sections such as controllers, services, configurations, and workers. Each component has a specific responsibility, ensuring maintainability.

2. **Task Queue Worker**: I set up a BullMQ-based worker to handle background task processing. This ensures that tasks are processed asynchronously without blocking the main API flow.

3. **Logging**: The task results are logged to a file (`task-logs.txt`) for tracking task completion. This log includes timestamps for each task processed.

4. **Rate Limiting of Task Processing**:
   - **Per-user Rate Limiting**: I implemented a rate-limiting system that restricts task processing based on the user's ID. This ensures that users can't overload the system by submitting tasks too frequently.
   - **Rate Limit Logic**:
     - I track the time of the last task processed for each user and the number of tasks processed within a given time window (stored in Redis).
     - If a user exceeds the allowed rate (e.g., more than 20 tasks per minute or 1 task per second), their tasks are requeued to avoid exceeding the limit.
     - Redis stores the rate limit state, ensuring persistence and consistency across processes.
   - **Dynamic Requeuing**: Tasks are automatically requeued if the rate limit is exceeded, ensuring they will be processed later.

### Scaling Further

1. **Cluster Setup**:
   - I used Node.js clustering to scale the system across available CPU cores. The clustering divides the workers into two sets:
     - **API Workers**: Handle incoming API requests and add tasks to the queue.
     - **Queue Workers**: Consume and process tasks from the task queue.

2. **Scaling Options**:
   - **Horizontal Scaling**: I can scale the application across multiple servers or containers. Each instance can have its own set of API and queue workers.
   - **Distributed Redis**: A Redis cluster could be used to handle larger workloads and ensure consistent rate-limiting across multiple application instances.
   - **Task Prioritization**: Adding task prioritization could help manage high-priority tasks more efficiently, while partitioning tasks by user groups would help balance the load across multiple workers.

### Cluster Replication

- **Cluster Setup**:
   - I divided the available CPU cores into two sets, one for API handling and the other for background task processing. 
     - **API Workers**: These handle incoming HTTP requests and add tasks to the queue.
     - **Queue Workers**: These are dedicated to processing tasks from the queue.

- **Cluster Forking Logic**:
   - The primary process forks workers based on the number of available CPU cores. The first half of the cores are assigned to handle API requests, while the second half handle background tasks.
   - This ensures that both API and task processing are handled efficiently without resource contention.

- **Worker Handling**:
   - **API Workers**: These create an HTTP server to handle requests (`createServer(app)`).
   - **Queue Workers**: These run the background worker for processing tasks (`taskWorker()`).

- **Resilience**: The cluster manager monitors worker processes. If a worker dies, a new one is spawned to replace it, ensuring the system remains operational.

This approach allows the system to efficiently handle high loads while distributing tasks across CPU cores. The use of clustering and Redis-backed rate-limiting ensures scalable, resilient task processing.

-------






